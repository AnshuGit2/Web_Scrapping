{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Python Web Scraping Introduction\n",
    "\n",
    "## Introduction - What Is Web Scraping?\n",
    "We have seen how many interesting Internet companies have evolved APIs to facilitate the delivery of\n",
    "data. While the availability of APIs for accessing data is on the rise, the fact remains that the vast\n",
    "majority of data available from the Web cannot be accessed via a well-formed API.\n",
    "\n",
    "Web scraping (WS) is a means of ingesting data from the web in the absence of an API. The objective of\n",
    "WS is to download semi-structured data from the web, select relevant portions of that data, and\n",
    "forwarding that data to the next stage in your analytics pipeline.\n",
    "WS projects can range from extremely sophisticated to “quick and dirty”. The good news is that the\n",
    "Python ecosystem provides some amazing tools in support of your Web scraping ambitions.\n",
    "\n",
    "## Web Scraping for Mere Mortals\n",
    "\n",
    "Recall that since all APIs are different there is an upfront investment (a learning curve) to using an API.\n",
    "You have to create developer accounts, understand API authentication and authorization, understand\n",
    "API endpoints, and data payloads, etc.\n",
    "WS also comes with its own learning curve. The first requirement of scraping any Web site is\n",
    "understanding the nature and structure of the HTML on the site’s web pages. That means looking at lots\n",
    "of HTML.\n",
    "\n",
    "When you decide to scrape a page you typically know what data from that page you hope to gather.\n",
    "Your job is to understand how the target data is nestled within the page’s HTML in order to devise an\n",
    "extraction plan. Needless to say, this is often a one-off (bespoke) plan that is unique to the page/site\n",
    "that you are scraping.\n",
    "\n",
    "## Research Question and Methodology\n",
    "Let’s get started. As with all analytics projects, you start with a question that needs answering and\n",
    "answering questions requires data. Here’s our question - `who are the five most popular actors to play\n",
    "the role of Dr. Who in the popular and long running BBC series Dr. Who?`\n",
    "\n",
    "Excellent question! Once we have a question, we need to decide what available data would be best to\n",
    "answer that question. In this case, lets agree that page views to an actor’s Wikipedia page will be our\n",
    "proxy for popularity.\n",
    "\n",
    "It’s always a good idea to consider how one would accomplish a task manually prior to automation.\n",
    "That’s not to say that they will be identical processes; however, they will be conceptually similar. In my\n",
    "mind, if I had a complete list of the Dr. Who actors, I could just Google each one,\n",
    "click on their Wikipedia link, then click on the Page information menu-link on the LHS of every Wikipedia\n",
    "web page. There I can see the number of page views in the last 30 days. Tally, those up for each actor,\n",
    "sort in descending order and, voilà, I have my answer.\n",
    "\n",
    "So, to implement this plan using a WS strategy I would need:\n",
    "\n",
    "1. a complete list of Dr. Who actors\n",
    "2. the Wikipedia page information page for each actor\n",
    "3. the 30-day page view information from that page\n",
    "\n",
    "## Web Scraping Solution\n",
    "This is where Python and Web scraping comes in. As stated above, WS is about downloading\n",
    "semi-structured data from the web, capturing some of that data, and using the data\n",
    "in an analytics pipeline.\n",
    "\n",
    "In this lab, you will be writing a Python program that:\n",
    "\n",
    "1. downloads the list of Dr. Who actors\n",
    "2. downloads their Wikipedia information pages\n",
    "3. captures the 30-day page view data as a proxy for their popularity\n",
    "4. record that data\n",
    "5. analyze the data\n",
    "\n",
    "## The Dr. Who Actors\n",
    "There many places to get the list of Dr. Who actors; however, one of my favorites is, of course,\n",
    "Entertainment Weekly (EW). Specifically, the following single page contains the complete list -\n",
    "https://ew.com/tv/doctor-who-actors/. Our job is to parse the HTML on the page to build our list.\n",
    "Navigate to the above page and use your browser to explore the HTML that underlies the page. For\n",
    "chromium browsers, this can be accomplished by pressing the ctrl+u keys. **What you are looking for are\n",
    "exploitable patterns that appear in the HTML.** This, is the essence (and the \"art\") of Web scraping.\n",
    "\n",
    "What does \"exploitable patterns\" mean exactly? Well, there are some number of actors and I\n",
    "do NOT want to write separate data extraction plans for each one – I want to write one!\n",
    "That means I’d like to find a pattern in the\n",
    "HTML surrounding the actor names that occurs for all of the actors. Usually, this is pretty easy\n",
    "when it comes to lists and this example is no exception.\n",
    "\n",
    "Looking at the rendered HTML via a browser (Ctrl+u) I see that the 13th Dr. Who is Jodie Whittaker.\n",
    "Now turning to the underlying HTML, I’m going to search for the string ‘Jodie Whittaker’. I’m going\n",
    "to continue to search until I see something that looks easy to parse and looks like a pattern\n",
    "that may repeat for all actors.\n",
    "\n",
    "Try this on your own before proceeding. It’s possible that we all come up with a different pattern!\n",
    "\n",
    "Here’s what I found. Look for an occurrence of Jodie Whittaker within a block of HTML that begins with\n",
    "&lt;noscript&gt;. I see the actor’s name in this block, so it looks promising. Next I will search for the HTML\n",
    "for &lt;noscript&gt; again to see if the other actors’ names also appear in similar blocks – they do.\n",
    "However, this pattern also occurs for other data on the page. Can you see a way to distinguish the\n",
    "‘useful’ HTML blocks (the ones that contain the data we are after) from the noise? The ability to discern\n",
    "these types of patterns are an essential skill for WS.\n",
    "\n",
    "Well, it turns out that if a &lt;img&gt; tag nested within a &lt;div&gt; tag nested with the &lt;noscript&gt; tag has a\n",
    "title attribute that begins with the regex pattern ^Slide\\s+\\d+: then we have our name. All we to do is\n",
    "find &lt;img&gt; tags that have title attributes that match this RE pattern!\n",
    "\n",
    "## Make a Web Request\n",
    "As with past labs, use the request library to return the content from the EW web page above. Ultimately,\n",
    "you will need the .text attribute from the response returned from HTTP GET. Refer to our previous API\n",
    "labs for a refresher if necessary.\n",
    "\n",
    "## Wrangling HTML with BeautifulSoup\n",
    "Once you have raw HTML in front of you, you can start to select and extract. For this purpose, we will be\n",
    "using BeautifulSoup(BS). The BS constructor parses raw HTML strings and produces an object that\n",
    "mirrors the HTML document’s structure. Parse? Yes, parse. That should sound familiar. That is the\n",
    "same term we applied to processing XML documents. In fact, HTML, like XML, is a tag-based language.\n",
    "BS parses HTML in exactly the same way as we parsed XML. The result of this parsing is an in memory\n",
    "tree of Python objects reflective of the HTML.\n",
    "\n",
    "### Consider the following example of an HTML document and the accompanying code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is testing\n",
      "['\\n', <p id=\"eggman\"> I am the egg man </p>, '\\n', <p id=\"walrus\"> I am the walrus </p>, '\\n']\n",
      "here is testing1\n",
      "[' I am the egg man ']\n",
      " I am the egg man \n",
      " I am the walrus \n",
      " I am the walrus \n",
      " I am the walrus \n"
     ]
    }
   ],
   "source": [
    "raw_html = '''\n",
    "<html>\n",
    "<head>\n",
    "    <title> Contrived Example\n",
    "</head>\n",
    "<body>\n",
    "    <p id=\"eggman\"> I am the egg man </p>\n",
    "    <p id=\"walrus\"> I am the walrus </p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "# The simplest way to navigate the parse tree is to say the name of the tag you want (akin to XPATH)\n",
    "# If you want the <body> tag, just say soup.body:\n",
    "body_tag = soup.body\n",
    "\n",
    "# A tag’s children are available in a list called .contents:\n",
    "print(\"here is testing\")\n",
    "print(body_tag.contents)\n",
    "\n",
    "# You can do use this trick again and again to zoom in on a certain part of the parse tree.\n",
    "# This code gets the FIRST <p> tag beneath the <body> tag:\n",
    "p_tag = body_tag.p\n",
    "\n",
    "# Note the diff between contents and text\n",
    "print(\"here is testing1\")\n",
    "print(p_tag.contents)\n",
    "print(p_tag.text)\n",
    "# If you need to get all the <a> tags, or anything more complicated than the first tag\n",
    "# with a certain name, you’ll need to use one of the 'searching' methods described in\n",
    "# Searching the tree (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree),\n",
    "# such as find_all():\n",
    "\n",
    "# Find ALL <p> tags in the soup - may be more than you want!\n",
    "for p in soup.find_all('p'):\n",
    "    if p['id'] == 'walrus':\n",
    "        print(p.text)\n",
    "\n",
    "# Like XML, You can find* from any valid element in the soup.\n",
    "# Here we start with the previously found body element\n",
    "#\n",
    "for p_tag in body_tag.find_all('p'):\n",
    "    if p_tag['id'] == 'walrus':\n",
    "        print(p_tag.text)\n",
    "\n",
    "# OR\n",
    "# some common HTML attributes like id, class, etc. are implemented\n",
    "# as key-word arguments\n",
    "for p in body_tag.find_all('p', id='walrus'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Breaking down the example, you first parse the raw HTML by passing it to the BS constructor. BS\n",
    "accepts multiple back-end parsers, but the standard back-end is html.parser, which you supply here as\n",
    "the second argument. Note; however, 'lxml' parser is also a popular choice for a parser.\n",
    "\n",
    "The 'tags-as-attributes' approach, allows you to find the first occurrence of a tag. This is\n",
    "a bit like XPATH.  This bit of magic is easy to implement in a Python class using the either the\n",
    "\\_\\_getattr\\_\\_ or \\_\\_getattribute\\_\\_ dunder methods.\n",
    "See https://medium.com/@satishgoda/python-attribute-access-using-getattr-and-getattribute-6401f7425ce6\n",
    "\n",
    "The issue with attribute approach however, is that you can only find the first\n",
    "such tag.  If you need to find all tags, the you need to use one of the the find* methods.\n",
    "See https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree.\n",
    "\n",
    "The find_all() method on the soup object lets you locate all matching elements in the document.\n",
    "While find() finds a single element.\n",
    "\n",
    "In the above case, soup.find_all('p') returns a list of\n",
    "paragraph elements (ie., &lt;p&gt;).\n",
    "Each *p* may have HTML attributes that you can access like a dictionary. In the line if p['id'] == 'walrus', for\n",
    "example, you check if the id attribute is equal to the string 'walrus', which corresponds to\n",
    "&lt;p id=\"walrus\"&gt; in the HTML. This is the same as what we saw when using ElementTree to parse XML.\n",
    "\n",
    "Also, as with XML, the .text property yields the text associated with a tag.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**Technical Note:** in addition to strings representing HTML tags, the arguments to find() and find_all() can\n",
    "be compiled regular expressions (RE). This is extremely useful as you shall see.\n",
    "<hr/>\n",
    "\n",
    "### Phase 1 - Using BeautifulSoup to Get Dr. Who Actor Names\n",
    "Now that you have given BeautifulSoup’s find methods in a short test drive, how do you\n",
    "determine exactly what argument to supply to find? The fastest way is to step out of Python use your\n",
    "browser to examine the underlying HTML of the document as discussed above.\n",
    "\n",
    "Recall that there is an &lt;img&gt; tag nested within a &lt;div&gt; tag nested with the &lt;noscript&gt; tag has a title\n",
    "attribute that, if it begins with the regex pattern ^Slide\\s+\\d+:\\s+[A-Z] , then this is an &lt;img&gt; tag we are\n",
    "interested in.\n",
    "\n",
    "The \"beautiful\" thing about BeautifulSoup is that many of the pattern-type arguments\n",
    "to BS methods can be compiled regular expressions. I told you this would be useful…\n",
    "\n",
    "So, to find &lt;img&gt; tags that contain the Dr. Who actor names we need a title attribute that matches the\n",
    "following compiled RE pattern:\n",
    "```\n",
    "re.compile(r'^Slide\\s+\\d+:\\s+[A-Z]')\n",
    "Ex title attribute data: Slide 15: First Doctor: William Hartnell\n",
    "```\n",
    "\n",
    "Once we have the title text, we will need to turn to REs again to get parse names coming from the title\n",
    "attribute – like so:\n",
    "```\n",
    " m = re.search(r'^Slide\\s+\\d+:[^:]+[:]\\s+(?P<actor>.*)$', title)\n",
    "```\n",
    "What’s going on in the above RE? We want the only the name from the title attribute text. This RE\n",
    "starts the same as before; however, after the first : the [^:]+ says \"gobble up all characters that are\n",
    "**not** a : (colon) until you run into a colon that is followed by one or more spaces.”\n",
    "\n",
    "After that, capture all remaining characters (the $ is the end-of-string anchor) in a group named <actor>.\n",
    "\n",
    "### To Do:\n",
    "To see how this part turned out, review the who_actors() function below.\n",
    "Make sure you understand what’s going on before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jo Martin', 'Jodie Whittaker', 'Peter Capaldi', 'Matt Smith', 'David Tennant', 'Christopher Eccleston', 'John Hurt', 'Paul McGann', 'Sylvester McCoy', 'Colin Baker', 'Peter Davison', 'Tom Baker', 'Jon Pertwee', 'Patrick Troughton', 'William Hartnell']\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "EW_URL = 'http://ew.com/tv/doctor-who-actors/'\n",
    "\n",
    "def simple_get(url, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, *args, **kwargs)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "        raise http_err\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "        raise err\n",
    "\n",
    "    return resp\n",
    "\n",
    "def who_actors(url):\n",
    "    resp = simple_get(url, timeout=5)\n",
    "    html = resp.text\n",
    "\n",
    "    # sanity check. is this HTML?\n",
    "    assert re.search('html', resp.headers['Content-Type'], re.IGNORECASE)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # to be returned\n",
    "    actor_list = []\n",
    "\n",
    "    for img in soup.find_all('img', title=re.compile(r'^Slide\\s+\\d+:\\s+[A-Z]')):\n",
    "\n",
    "        # I want the name from the title attribute which looks like this:\n",
    "        # Slide 10: Sixth Doctor: Colin Baker\n",
    "        # Another good use for REs.\n",
    "        # This RE starts the same as before; however, after the first :\n",
    "        # the [^:]+[:]\\s+ says \"gobble up all (one ore more) characters that\n",
    "        # are not a : until you run into a colon\n",
    "        # that is followed by one or more spaces. After that,\n",
    "        # capture all remaining characters in a group named <actor>\"\n",
    "        #\n",
    "        title = img['title']\n",
    "\n",
    "        m = re.search(r'^Slide\\s+\\d+:[^:]+[:]\\s+(?P<actor>.*)$', title)\n",
    "        # if no match, then I've screwed up something\n",
    "        assert m is not None\n",
    "        if m:\n",
    "            actor_list.append(m.group('actor'))\n",
    "\n",
    "    # Great, got my list of actors. Return to caller\n",
    "    return actor_list\n",
    "\n",
    "# PHASE 1: TESTING ONLY\n",
    "# Get the Dr.Who actors from EW_URL\n",
    "alst = who_actors(EW_URL)\n",
    "print(alst)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
