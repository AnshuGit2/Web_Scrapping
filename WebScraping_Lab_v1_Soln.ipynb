{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Python Web Scraping\n",
    "In this lab, you are to continue to build on the Dr. Who popularity solution.  What remains\n",
    "is to evaluate the popularity of each Dr. Who actor by\n",
    "using the page views of the actor’s Wikipedia page as a proxy for their popularity.\n",
    "\n",
    "##  Using the Names + BeautifulSoup the Get the Stats\n",
    "Using the exact same principles used to collect the list of Dr. Who actors,\n",
    "we now need to collect the 30-day page view stat for each actor.\n",
    "\n",
    "The pseudocode for this activity is roughly as follows:\n",
    "\n",
    "1. Explore the HTML underlying an example Wikipedia stats page:\n",
    "https://en.wikipedia.org/w/index.php?title=Jodie_Whittaker&action=info\n",
    "Look (**hard**) for a pattern that will allow you to capture the Page views in the past 30 days.\n",
    "Turns out there is perfect pattern you should be able to exploit.\n",
    "2. For each actor, combine the actor name with the Wikipedia URL string as a parameter\n",
    " - Fetch the stats web page by GET(ting) the URL just constructed\n",
    " - Parse the returned HTML using Beautiful Soup\n",
    " - Find the stats using your previously observed exploitable pattern\n",
    " - remove any noise from the stats string number\n",
    " - convert stats string to integer via int()\n",
    " - track the actor’s stat using a list or dictionary\n",
    "3. Sort the actor stats in descending order\n",
    "4. print the top 5\n",
    "\n",
    "Have a beer – you deserve it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drum roll please...\n",
      "The top 5 Dr. Who actors are:\n",
      "\tDavid Tennant : 120349\n",
      "\tMatt Smith : 113168\n",
      "\tJodie Whittaker : 94588\n",
      "\tJohn Hurt : 87538\n",
      "\tChristopher Eccleston : 86257\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "EW_URL = 'http://ew.com/tv/doctor-who-actors/'\n",
    "\n",
    "def simple_get(url, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, *args, **kwargs)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "        raise http_err\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "        raise err\n",
    "\n",
    "    return resp\n",
    "\n",
    "def who_actors(url):\n",
    "    resp = simple_get(url, timeout=5)\n",
    "    html = resp.text\n",
    "\n",
    "    # sanity check. is this HTML?\n",
    "    assert re.search('html', resp.headers['Content-Type'], re.IGNORECASE)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # to be returned\n",
    "    actor_list = []\n",
    "\n",
    "    for img in soup.find_all('img', title=re.compile(r'^Slide\\s+\\d+:\\s+[A-Z]')):\n",
    "\n",
    "        # I want the name from the title attribute which looks like this:\n",
    "        # Slide 10: Sixth Doctor: Colin Baker\n",
    "        # Another good use for REs.\n",
    "        # This RE starts the same as before; however, after the first :\n",
    "        # the [^:]+[:]\\s+ says \"gobble up all (one ore more) characters that\n",
    "        # are not a : until you run into a colon\n",
    "        # that is followed by one or more spaces. After that,\n",
    "        # capture all remaining characters in a group named <actor>\"\n",
    "        #\n",
    "        title = img['title']\n",
    "\n",
    "        m = re.search(r'^Slide\\s+\\d+:[^:]+[:]\\s+(?P<actor>.*)$', title)\n",
    "        # if no match, then I've screwed up something\n",
    "        assert m is not None\n",
    "        if m:\n",
    "            actor_list.append(m.group('actor'))\n",
    "\n",
    "    # Great, got my list of actors. Return to caller\n",
    "    return actor_list\n",
    "\n",
    "'''\n",
    "    # PHASE 2:\n",
    "    # Collect the stats from Wikipedia\n",
    "    # for each who actor\n",
    "'''\n",
    "\n",
    "def who_stats(dr_who):\n",
    "    url = 'https://en.wikipedia.org/w/index.php'\n",
    "\n",
    "    # Notice that navigation to the info page is a query param\n",
    "    resp = simple_get(url, params={'title':dr_who, 'action': 'info'})\n",
    "    # get the decoded payload.  the text() method uses metadata to devine encoding.\n",
    "    html = resp.text\n",
    "\n",
    "    # By inspection of HTTP results you will find that the\n",
    "    # stat we seek is extremely easy to find:\n",
    "    # <div class=\"mw-pvi-month\">58,243</div>\n",
    "    # the <div> tag has a class attr designed to display the \"pvi\" - page view in...months!\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #  Only need a find (not find_all) since there is only a single tag\n",
    "    # that has a class attr = mw-pvi-month\n",
    "\n",
    "    div = soup.find('div', class_='mw-pvi-month')\n",
    "    # sanity check\n",
    "    assert div is not None\n",
    "    # this text may have commas which need to be removed\n",
    "    # prior to parsing as an int\n",
    "    return int(div.text.replace(',',''))\n",
    "\n",
    "def main():\n",
    "    # PHASE 1:\n",
    "    # Get the Dr.Who actors from EW_URL\n",
    "    actor_list = who_actors(EW_URL)\n",
    "\n",
    "    # PHASE 2:\n",
    "    # Collect the stats from Wikipedia\n",
    "    # for each who actor\n",
    "    #\n",
    "    actor_stats_dict = {}\n",
    "\n",
    "    for a in actor_list:\n",
    "        # the names from the EW are separated by \\s chars.  In wikipedia urls,\n",
    "        # those spaces need to become underscores (_)\n",
    "        wiki_a = a.replace(' ', '_')\n",
    "        pvim_stat = who_stats(wiki_a)\n",
    "        actor_stats_dict[a] = pvim_stat\n",
    "\n",
    "    # PHASE 3:\n",
    "    # Sort number of views in desc order\n",
    "    sorted_actor_list = sorted(actor_stats_dict, key=actor_stats_dict.get, reverse=True)\n",
    "\n",
    "    print(\"Drum roll please...\\nThe top 5 Dr. Who actors are:\")\n",
    "    for a in sorted_actor_list[0:5]:\n",
    "        cnt = actor_stats_dict[a]\n",
    "        print(f'\\t{a} : {cnt}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
